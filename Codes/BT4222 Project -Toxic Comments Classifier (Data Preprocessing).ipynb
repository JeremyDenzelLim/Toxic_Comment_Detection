{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BT4222 Project -Toxic Comments Classifier (Data Preprocessing)\n",
    "\n",
    "In this notebook, we will perform preprocessing such as removal of stopwords, emoji in the text. For full elaboration on the process, please refer to the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-01T19:57:10.679694Z",
     "start_time": "2019-11-01T19:57:10.000958Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-01T19:57:10.933082Z",
     "start_time": "2019-11-01T19:57:10.929862Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-01T19:57:12.378309Z",
     "start_time": "2019-11-01T19:57:11.305848Z"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-01T20:03:35.395800Z",
     "start_time": "2019-11-01T20:03:35.356395Z"
    }
   },
   "outputs": [],
   "source": [
    "def Sanitize(df, comment_text):\n",
    "    emoji_dict = defaultdict()\n",
    "    with io.open('emoji_unicode_names_final.txt', 'r', encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            emoji_dict[tokens[0]] = tokens[1]\n",
    "    repl = {\n",
    "    \"&lt;3\": \" good \",\n",
    "    \":d\": \" good \",\n",
    "    \":dd\": \" good \",\n",
    "    \":p\": \" good \",\n",
    "    \"8)\": \" good \",\n",
    "    \":-)\": \" good \",\n",
    "    \":)\": \" good \",\n",
    "    \";)\": \" good \",\n",
    "    \"(-:\": \" good \",\n",
    "    \"(:\": \" good \",\n",
    "    \"yay!\": \" good \",\n",
    "    \"yay\": \" good \",\n",
    "    \"yaay\": \" good \",\n",
    "    \"yaaay\": \" good \",\n",
    "    \"yaaaay\": \" good \",\n",
    "    \"yaaaaay\": \" good \",\n",
    "    \":/\": \" bad \",\n",
    "    \":&gt;\": \" sad \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" bad \",\n",
    "    \":(\": \" bad \",\n",
    "    \":s\": \" bad \",\n",
    "    \":-s\": \" bad \",\n",
    "    \"&lt;3\": \" heart \",\n",
    "    \":d\": \" smile \",\n",
    "    \":p\": \" smile \",\n",
    "    \":dd\": \" smile \",\n",
    "    \"8)\": \" smile \",\n",
    "    \":-)\": \" smile \",\n",
    "    \":)\": \" smile \",\n",
    "    \";)\": \" smile \",\n",
    "    \"(-:\": \" smile \",\n",
    "    \"(:\": \" smile \",\n",
    "    \":/\": \" worry \",\n",
    "    \":&gt;\": \" angry \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" sad \",\n",
    "    \":(\": \" sad \",\n",
    "    \":s\": \" sad \",\n",
    "    \":-s\": \" sad \",\n",
    "    r\"\\br\\b\": \"are\",\n",
    "    r\"\\bu\\b\": \"you\",\n",
    "    r\"\\bhaha\\b\": \"ha\",\n",
    "    r\"\\bhahaha\\b\": \"ha\",\n",
    "    r\"\\bdon't\\b\": \"do not\",\n",
    "    r\"\\bdoesn't\\b\": \"does not\",\n",
    "    r\"\\bdidn't\\b\": \"did not\",\n",
    "    r\"\\bhasn't\\b\": \"has not\",\n",
    "    r\"\\bhaven't\\b\": \"have not\",\n",
    "    r\"\\bhadn't\\b\": \"had not\",\n",
    "    r\"\\bwon't\\b\": \"will not\",\n",
    "    r\"\\bwouldn't\\b\": \"would not\",\n",
    "    r\"\\bcan't\\b\": \"can not\",\n",
    "    r\"\\bcannot\\b\": \"can not\",\n",
    "    r\"\\bi'm\\b\": \"i am\",\n",
    "    \"m\": \"am\",\n",
    "    \"r\": \"are\",\n",
    "    \"u\": \"you\",\n",
    "    \"haha\": \"ha\",\n",
    "    \"hahaha\": \"ha\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"cannot\": \"can not\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"m\": \"am\",\n",
    "    \"i'll\" : \"i will\",\n",
    "    \"its\" : \"it is\",\n",
    "    \"it's\" : \"it is\",\n",
    "    \"'s\" : \" is\",\n",
    "    \"that's\" : \"that is\",\n",
    "    \"weren't\" : \"were not\",\n",
    "    }\n",
    "\n",
    "    keys = [i for i in repl.keys()]\n",
    "    ltr = df[comment_text].tolist()\n",
    "    new_train_data = []\n",
    "    stopwords_en = stopwords.words('english')\n",
    "    for i in ltr:\n",
    "        arr = str(i).split()\n",
    "        xx = \"\"\n",
    "        for j in arr:\n",
    "            j = str(j).lower()\n",
    "            if j[:4] == 'http' or j[:3] == 'www':\n",
    "                continue\n",
    "            if j in keys:\n",
    "                # print(\"inn\")\n",
    "                j = repl[j]\n",
    "            if j in emoji_dict:\n",
    "                j = emoji_dict[j]\n",
    "            xx += j + \" \"\n",
    "        new_train_data.append(xx)\n",
    "    df[comment_text] = new_train_data\n",
    "    \n",
    "    comment = comment_text\n",
    "    \n",
    "    df[comment] = df[comment].str.lower().str.replace('newline_token', ' ')\n",
    "    df[comment] = df[comment].fillna('erikov')\n",
    "    #special symbols\n",
    "    df[comment] = df[comment].apply(lambda x : \" \".join(re.findall('[\\w]+',x)))\n",
    "    #stop words\n",
    "    #df[comment] = df[comment].apply(lambda x: self.remove_stopwords(x, stopwords_en))\n",
    "    #websites\n",
    "    df[comment] = df[comment].replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
    "    df[comment] = df[comment].replace(r'@[^\\s]+[\\s]?', '', regex=True)\n",
    "    #ip address\n",
    "    df[comment] = df[comment].replace(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', '', regex=True)\n",
    "    \n",
    "    df[comment] = df[comment].replace('&', ' and ', regex=True)\n",
    "    df[comment] = df[comment].replace('@', ' at ', regex=True)\n",
    "    df[comment] = df[comment].replace('0', ' zero ', regex=True)\n",
    "    df[comment] = df[comment].replace('1', ' one ', regex=True)\n",
    "    df[comment] = df[comment].replace('2', ' two ', regex=True)\n",
    "    df[comment] = df[comment].replace('3', ' three ', regex=True)\n",
    "    df[comment] = df[comment].replace('4', ' four ', regex=True)\n",
    "    df[comment] = df[comment].replace('5', ' five ', regex=True)\n",
    "    df[comment] = df[comment].replace('6', ' six ', regex=True)\n",
    "    df[comment] = df[comment].replace('7', ' seven ', regex=True)\n",
    "    df[comment] = df[comment].replace('8', ' eight ', regex=True)\n",
    "    df[comment] = df[comment].replace('9', ' nine ', regex=True)\n",
    "    #multi space\n",
    "    df[comment] = df[comment].replace('\\s+', ' ', regex=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sanitize2(df,comment_text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words = stop_words + [\"br\",\"a\"] \n",
    "        \n",
    "    text_data = df[comment_text].tolist()\n",
    "    text_data = [simple_preprocess(text,deacc=True) for text in text_data] \n",
    "    text_data = [[token for token in text if token not in stop_words] for text in text_data]\n",
    "    text_data = [\" \".join(text) for text in text_data]\n",
    "    df[comment_text] = text_data\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-01T20:03:36.575212Z",
     "start_time": "2019-11-01T20:03:35.825715Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-01T20:04:01.445349Z",
     "start_time": "2019-11-01T20:03:36.607765Z"
    }
   },
   "outputs": [],
   "source": [
    "df = Sanitize(df, 'comment_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Sanitize2(df, 'comment_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"cleaned_train\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = Sanitize(df_test, 'comment_text')\n",
    "df_test = Sanitize2(df_test, 'comment_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(\"cleaned_test\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
